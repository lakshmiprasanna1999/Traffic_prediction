{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traffic_Volume.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshmiprasanna1999/Traffic_prediction/blob/master/Traffic_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8sR81QEGV7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuY7X5Z2G7hT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzCs92_nD7Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/wshuyi/demo_traffic_jam_prediction.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87m911FsG7bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "data_dir = Path('demo_traffic_jam_prediction')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZCSeNdJPq5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(data_dir / 'data.pickle', 'rb') as f:\n",
        "    [event_dict, df] = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66qN6dQiG7U2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "event_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49y2Cx_cNGPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWW-ijGzNMrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZMmIcrhNTKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_event_id = df.events.apply(len).idxmax()\n",
        "max_len_event_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKHF3-daNasF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_event = df.iloc[max_len_event_id]\n",
        "max_len_event.events"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6iR_LjwNanJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = len(max_len_event.events)\n",
        "maxlen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKlPDU6HNamK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reversed_dict = {}\n",
        "for k, v in event_dict.items():\n",
        "  reversed_dict[v] = k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK77VQsuNagg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reversed_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YLLNk7HN4B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_event_list_to_idxs(event_list):\n",
        "  list_idxs = []\n",
        "  for event in (event_list):\n",
        "    idx = reversed_dict[event]\n",
        "    list_idxs.append(idx)\n",
        "  return list_idxs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_abbuXw0N383",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "map_event_list_to_idxs(max_len_event.events)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2s6ao-ON374",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr2sjrkCOMK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(event_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNs_t5MOOROq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.events.apply(map_event_list_to_idxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LjOP18UOX5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = df.events.apply(map_event_list_to_idxs).tolist()\n",
        "sequences[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jijrS00OQMS2",
        "colab_type": "text"
      },
      "source": [
        " the first row is much longer than the following ones.\n",
        "However, to apply a sequence model on the data, we need to make sure all the input sequences share the same length. Hence, we use the length of the longest sequence as the max length, and fill other shorter sequences with 0s from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyF2c1QzOd9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmBe4qRVOdxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = np.array(df.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnt-28ZqOrMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVRL8NHrPH0j",
        "colab_type": "text"
      },
      "source": [
        "We shuffle the sequences along with their corresponding labels.as we are done with the running of the code so we can make changes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G58_urgwOsGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_dYJlfrO-26",
        "colab_type": "text"
      },
      "source": [
        "The training set will contain 80% of the data, while the other 20% goes into the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQbO0-YkOsFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_samples = int(len(indices) * .8)\n",
        "validation_samples = len(indices) - training_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG1XvS3EPcLB",
        "colab_type": "text"
      },
      "source": [
        "The following codes divide the data into training and validation sets, along with the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L62o6qg1PXtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "X_valid = data[training_samples: training_samples + validation_samples]\n",
        "y_valid = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3tJ_xiQPmGN",
        "colab_type": "text"
      },
      "source": [
        "the content of training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YJAwq6_PXrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDQ6H7gzPypk",
        "colab_type": "text"
      },
      "source": [
        " as we filled the sequences with 0 as padding value, now we have 33, instead of 32 event types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSitDxkeP4Kq",
        "colab_type": "text"
      },
      "source": [
        "So the number of event types will be set to 33. here we add +1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK-_5sL9PXgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_events = len(event_dict) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BWiWUU4QfBK",
        "colab_type": "text"
      },
      "source": [
        "If we simply put the numbers into classification model, it will regard each number as a continuous value. However, they are not. So we will let the numbers go through an Embedding layer, and convert each number (representing a certain type of event) into a vector. Each vector, will contain 20 scalars."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdTL7wdKQj43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5372rZwGQucp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The initial embedding matrix will be generated randomly.\n",
        "embedding_matrix = np.random.rand(num_events, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzLEJi5YQ7_C",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can build a model now.\n",
        "We use the Sequential model in Keras, and put different layers one by one, as we play with legos.\n",
        "The first layer is Embedding Layer, then a LSTM Layer follows, the last layer is a dense one, whose activation function is sigmoid, to make binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytze9CjSQ3X1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense, LSTM\n",
        "\n",
        "units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_events, embedding_dim))\n",
        "model.add(LSTM(units))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX6HCmbbRLtV",
        "colab_type": "text"
      },
      "source": [
        "The next step is to handle the parameters in the Embedding layer. For now, we just load in the initial embedding matrix generated randomly, and won’t let the training process change the weights in Embedding Layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_Z341coRLE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttf4vn_WRe2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Then, we train the model, and save the model into a h5 file.\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "model.save(\"mymodel_embedding_untrainable.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaf5JxH0R_4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#After the model is trained, let us visualize the curves of accuracy and loss with matplotlib.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHjcYQLeSZwU",
        "colab_type": "text"
      },
      "source": [
        "As you can see, it is not bad. If we use a dummy model to predict everything as label 0 (or all as 1), the accuracy will stay at 0.50. So our model, apparently, has captured some pattern, and out-performed the dummy one.\n",
        "However, it is very unstable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjRBRLFKSvrF",
        "colab_type": "text"
      },
      "source": [
        "LOSS :-------------------\n",
        "As you may find out, it is not good. When the loss of training went down, the loss on validation set bumped, and there is no significant trend of convergence.\n",
        "It is more important to find out the reason.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLkbtkxfTJ3T",
        "colab_type": "text"
      },
      "source": [
        "Note that we used a randomly initialized Embedding Matrix which stayed static during the training phase. It may lead us into trouble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5rtiMTmTD65",
        "colab_type": "text"
      },
      "source": [
        "So next step, we can do an experiment to allow the Embedding layer be trained and adjusted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mRI8MBzTvbc",
        "colab_type": "text"
      },
      "source": [
        "HERE WE JUST SET IT AS TRUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2PurfrDS51n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense, LSTM\n",
        "\n",
        "units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_events, embedding_dim))\n",
        "model.add(LSTM(units))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLTye1TST0sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The only different in the code, is that parameter trainable was set to True.\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9stDtxE0UFVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "model.save(\"mymodel_embedding_trainable.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX50up9rUaAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OmAI_ntVJk3",
        "colab_type": "text"
      },
      "source": [
        "As you can see, it got better. The fluctuation of validation accuracy curve went down, while the validation accuracy got higher than 0.75.\n",
        "This model is, to some extent, more valuable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsPDTPP7VWtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We will add two parameters related with Dropouts. To do this, we use dropout=0.2, recurrent_dropout=0.2 when defining the LSTM layer.\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense, LSTM\n",
        "\n",
        "units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_events, embedding_dim))\n",
        "model.add(LSTM(units, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX2kSkAcVd-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We will keep the parameter trainable of Embedding Layer to True.\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgTPHPhnVk9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "model.save(\"mymodel_embedding_trainable_with_dropout.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zVWo49wWQMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQJWLij6We3B",
        "colab_type": "text"
      },
      "source": [
        "when you look into the curve of loss, you’ll see significant improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVR7gmhcWy5Q",
        "colab_type": "text"
      },
      "source": [
        "The curve of validation loss is smoother, and much closer to the trend of training loss.\n",
        "Over-fitting has been taken care of, and the model is now more stable and generalizable to unseen data.\n",
        "The Traffic Administration can then use the model to predict the happening of severe traffic volume with the Waze open data of incidents report. The expectation of model accuracy is about 75%."
      ]
    }
  ]
}